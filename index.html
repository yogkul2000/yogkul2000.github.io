<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Yogesh Kulkarni</title> <meta name="author" content="Yogesh Kulkarni"> <meta name="description" content=""> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://yogkul2000.github.io//"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <div class="navbar-brand social"> <a href="mailto:%79%6B%75%6C%6B%61%31%30@%61%73%75.%65%64%75" title="email"><i class="fas fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=_GCLk8UAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://www.linkedin.com/in/ykulkarn" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fab fa-linkedin"></i></a> </div> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">papers</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div style="max-width: 800px; margin: 0 auto; text-align: justify;"> <div class="post"> <header class="post-header" style="text-align: center;"> <img src="assets/img/prof_pic.jpg" alt="prof_pic.jpg" class="img-fluid rounded-circle" style="max-width: 350px; margin-bottom: 20px;"> <h1 class="post-title" style="font-size: 2.8em; margin-bottom: 20px;"> <span class="font-weight-bold">Yogesh</span> Kulkarni </h1> <p class="desc"></p> </header> <article> <div class="clearfix"> <p>I am a first-year Computer Science Ph.D. student at the <strong><a href="https://scai.engineering.asu.edu/" rel="external nofollow noopener" target="_blank">School of Computing and Augmented Intelligence (SCAI)</a></strong>, <strong><a href="https://www.asu.edu/" rel="external nofollow noopener" target="_blank">Arizona State University</a></strong>, advised by <strong><a href="https://search.asu.edu/profile/4408705" rel="external nofollow noopener" target="_blank">Dr. Pooyan Fazli</a></strong> and part of the <strong><a href="https://www.pooyanfazli.com/index.html" rel="external nofollow noopener" target="_blank">People and Robots Laboratory (PeRL)</a></strong>. My research revolves around extending Large Vision Language Models (LVLMS) for Video Understanding and aligning with human preferences using Reinforcement Learning.</p> <p>Previously, I graduated from the <strong><a href="https://www.cs.usc.edu/" rel="external nofollow noopener" target="_blank">University of Southern California (USC)</a></strong> with a Master’s in Computer Science. At USC, I was a Graduate Research Assistant at the <strong><a href="https://ict.usc.edu/" rel="external nofollow noopener" target="_blank">USC Institute for Creative Technologies (ICT)</a></strong>, where I worked with 3D Point Clouds, particularly at the intersection of GANs, Diffusion Models, and Gaussian Splatting for style transfer in the 3D domain. In Summer 2023, I had the privilege to intern at <strong><a href="https://www.bell-labs.com/#gref" rel="external nofollow noopener" target="_blank">Nokia Bell Labs</a></strong>, contributing to efficient geo-distributed LLM training across heterogeneous clusters.</p> <p>Before coming to the US, I graduated from the <strong><a href="https://pict.edu/" rel="external nofollow noopener" target="_blank">Pune Institute of Computer Technology</a></strong> with a Bachelor’s in Computer Engineering. I was born and raised in New Delhi, where I spent 14 years at <strong><a href="https://mountcarmelschool.com/our-schools/" rel="external nofollow noopener" target="_blank">Mount Carmel School</a></strong>.</p> <p><span style="font-size: 1em; font-style: italic; color: #666; font-weight: 300;">“Sometimes the best journeys in life are the ones we never expected to take.”</span> <span style="font-size: 0.8em; color: #888; font-weight: 300;">~ Unknown</span></p> </div> <h2><a href="/publications/" style="color: inherit;">Selected papers</a></h2> <div class="publications"> <h2 class="bibliography">2024</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/pipeline-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/pipeline-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/pipeline-1400.webp"></source> <img src="/assets/img/publication_preview/pipeline.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="pipeline.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="kulkarni2024videosavi" class="col-sm-8"> <div class="title">VideoSAVi: Self-Aligned Video Language Models without Human Supervision</div> <div class="author"> <em>Yogesh Kulkarni</em>, and Pooyan Fazli</div> <div class="periodical"> <em>arXiv preprint arXiv:2412.00624</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2412.00624" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/videosavi.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Recent advances in vision-language models (VLMs) have significantly enhanced video understanding tasks. Instruction tuning (i.e., fine-tuning models on datasets of instructions paired with desired outputs) has been key to improving model performance. However, creating diverse instruction-tuning datasets is challenging due to high annotation costs and the complexity of capturing temporal information in videos. Existing approaches often rely on large language models to generate instruction-output pairs, which can limit diversity and lead to responses that lack grounding in the video content. To address this, we propose VideoSAVi (Self-Aligned Video Language Model), a novel self-training pipeline that enables VLMs to generate their own training data without extensive manual annotation. The process involves three stages: (1) generating diverse video-specific questions, (2) producing multiple candidate answers, and (3) evaluating these responses for alignment with the video content. This self-generated data is then used for direct preference optimization (DPO), allowing the model to refine its own high-quality outputs and improve alignment with video content. Our experiments demonstrate that even smaller models (0.5B and 7B parameters) can effectively use this self-training approach, outperforming previous methods and achieving results comparable to those trained on proprietary preference data. VideoSAVi shows significant improvements across multiple benchmarks: up to 28% on multi-choice QA, 8% on zero-shot open-ended QA, and 12% on temporal reasoning benchmarks. These results demonstrate the effectiveness of our self-training approach in enhancing video understanding while reducing dependence on proprietary models.</p> </div> </div> </div> </li></ol> </div> <h2><a href="/news/" style="color: inherit;">News</a></h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row">Aug 5, 2024</th> <td> Joined <strong><a href="https://scai.engineering.asu.edu/" rel="external nofollow noopener" target="_blank">(SCAI)</a></strong>, <strong><a href="https://www.asu.edu/" rel="external nofollow noopener" target="_blank">Arizona State University</a></strong> for my PhD in Computer Science <img class="emoji" title=":nerd_face:" alt=":nerd_face:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f913.png" height="20" width="20"> </td> </tr> <tr> <th scope="row">May 10, 2024</th> <td> Graduated from <strong><a href="https://drive.google.com/file/d/1HgK3l6Y3HlF6A9b5__AbiEZoR_3csquU/view?usp=sharing" rel="external nofollow noopener" target="_blank">University of Southern California (USC)</a></strong> with Master’s in CS (GPA = 3.67) <img class="emoji" title=":pleading_face:" alt=":pleading_face:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f97a.png" height="20" width="20"> </td> </tr> <tr> <th scope="row">Jun 5, 2023</th> <td> Joined <strong><a href="https://drive.google.com/file/d/1rl8UMPv9ftySwJfmPxDPsGAmBvaHfQbB/view?usp=sharing" rel="external nofollow noopener" target="_blank">Nokia Bell Labs</a></strong> as Research Intern working on scaling training of LLMs <img class="emoji" title=":heart_eyes:" alt=":heart_eyes:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f60d.png" height="20" width="20"> </td> </tr> <tr> <th scope="row">Jan 19, 2023</th> <td> Joined <strong><a href="https://drive.google.com/file/d/1WbyMURs3_mnSYL111jU6_SavPfC4VhC1/view?usp=sharing" rel="external nofollow noopener" target="_blank">USC Institute for Creative Technologies (ICT)</a></strong> as graduate RA working in 3D Vision <img class="emoji" title=":sweat_smile:" alt=":sweat_smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f605.png" height="20" width="20"> </td> </tr> <tr> <th scope="row">Aug 22, 2022</th> <td> Joined <strong><a href="https://drive.google.com/file/d/1kgYWM-t0annOFY2BFw7oZsFQjZ-bmHqk/view?usp=sharing" rel="external nofollow noopener" target="_blank">University of Southern California (USC)</a></strong> for my Master’s in Computer Science <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20"> </td> </tr> <tr> <th scope="row">Jul 31, 2022</th> <td> Graduated from <strong><a href="https://drive.google.com/file/d/1ik5UBovS_wLfB6cM-MCqWUUJo4zjmkLJ/view?usp=sharing" rel="external nofollow noopener" target="_blank">Pune Institute of Computer Technology</a></strong> with CGPA of 9.8 <img class="emoji" title=":pray:" alt=":pray:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f64f.png" height="20" width="20"> </td> </tr> <tr> <th scope="row">Jan 6, 2022</th> <td> Invited to Google Research India Graduate Symposium <img class="emoji" title=":sunglasses:" alt=":sunglasses:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f60e.png" height="20" width="20"> </td> </tr> <tr> <th scope="row">Jan 3, 2022</th> <td> Joined <strong><a href="https://www.episource.com/" rel="external nofollow noopener" target="_blank">Episource LLC</a></strong> as NLP Intern working on word sense disambiguation <img class="emoji" title=":blush:" alt=":blush:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f60a.png" height="20" width="20"> </td> </tr> <tr> <th scope="row">Dec 20, 2021</th> <td> Awarded Conference Travel Grant for ICDM ‘21 <img class="emoji" title=":pleading_face:" alt=":pleading_face:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f97a.png" height="20" width="20"> </td> </tr> <tr> <th scope="row">Sep 26, 2021</th> <td> <strong><a href="https://arxiv.org/abs/2110.04502" rel="external nofollow noopener" target="_blank">EnsembleNTLDetect</a></strong> accepted at ICDM ‘21 (workshops) <img class="emoji" title=":pleading_face:" alt=":pleading_face:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f97a.png" height="20" width="20"> </td> </tr> <tr> <th scope="row">Jul 1, 2021</th> <td> Joined <strong><a href="https://rbcdsai.iitm.ac.in/" rel="external nofollow noopener" target="_blank">RBCDSAI (IIT Madras)</a></strong> as a Research Intern working on detecting electricity theft <img class="emoji" title=":pleading_face:" alt=":pleading_face:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f97a.png" height="20" width="20"> </td> </tr> <tr> <th scope="row">May 18, 2021</th> <td> <strong><a href="https://arxiv.org/abs/2108.10251" rel="external nofollow noopener" target="_blank">Kryptonite paper</a></strong> accepted at ACNS ‘21 <img class="emoji" title=":pleading_face:" alt=":pleading_face:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f97a.png" height="20" width="20"> </td> </tr> <tr> <th scope="row">Apr 15, 2021</th> <td> Joined <strong><a href="https://cse.iitj.ac.in/" rel="external nofollow noopener" target="_blank">IIT Jodhpur</a></strong> as a Research Intern working on detecting urban anomalies <img class="emoji" title=":grin:" alt=":grin:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f601.png" height="20" width="20"> </td> </tr> <tr> <th scope="row">Dec 16, 2020</th> <td> Awarded Conference Travel Grant for IEEE Big Data’20 conference <img class="emoji" title=":pleading_face:" alt=":pleading_face:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f97a.png" height="20" width="20"> </td> </tr> <tr> <th scope="row">Nov 10, 2020</th> <td> 1 <strong><a href="https://ieeexplore.ieee.org/abstract/document/9377974" rel="external nofollow noopener" target="_blank">paper</a></strong> accepted at IEEE Big Data ‘20 <img class="emoji" title=":partying_face:" alt=":partying_face:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f973.png" height="20" width="20"> </td> </tr> <tr> <th scope="row">Oct 20, 2020</th> <td> Joined <strong><a href="https://www.omdena.com/" rel="external nofollow noopener" target="_blank">Omdena</a></strong> as a volunteer machine learning engineer working on AI for social good <img class="emoji" title=":raised_hands:" alt=":raised_hands:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f64c.png" height="20" width="20"> </td> </tr> <tr> <th scope="row">Sep 5, 2020</th> <td> Joined PICT’s Computational Linguistic Lab working on automatic code-completion <img class="emoji" title=":innocent:" alt=":innocent:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f607.png" height="20" width="20"> </td> </tr> <tr> <th scope="row">Jul 1, 2020</th> <td> Joined <strong><a href="https://www.drdo.gov.in/" rel="external nofollow noopener" target="_blank">DRDO HQ</a></strong> as a Research Intern working on ML for malware analysis <img class="emoji" title=":grin:" alt=":grin:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f601.png" height="20" width="20"> </td> </tr> <tr> <th scope="row">Aug 1, 2018</th> <td> Started my undergrad in Computer Engineering at <strong><a href="https://pict.edu/" rel="external nofollow noopener" target="_blank">Pune Institute of Computer Technology</a></strong> <img class="emoji" title=":clap:" alt=":clap:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f44f.png" height="20" width="20"> </td> </tr> </table> </div> </div> </article> </div> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0" style="color: white;"> © Copyright 2024 Yogesh Kulkarni. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>